{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":128343,"databundleVersionId":15390938,"sourceType":"competition"},{"sourceId":14571098,"sourceType":"datasetVersion","datasetId":9307592}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:12:39.678179Z","iopub.execute_input":"2026-01-24T18:12:39.678389Z","iopub.status.idle":"2026-01-24T18:12:43.672276Z","shell.execute_reply.started":"2026-01-24T18:12:39.678368Z","shell.execute_reply":"2026-01-24T18:12:43.671349Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.6.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.0)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (26.0rc2)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\nRequirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os, random, gc\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup, AutoConfig\nfrom tqdm import tqdm\n\nimport optuna\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:12:43.673949Z","iopub.execute_input":"2026-01-24T18:12:43.674201Z","iopub.status.idle":"2026-01-24T18:12:58.651742Z","shell.execute_reply.started":"2026-01-24T18:12:43.674170Z","shell.execute_reply":"2026-01-24T18:12:58.651166Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"COMP_PATH = Path(\"/kaggle/input/deep-learning-for-computer-vision-and-nlp-2026-01\")\nFOLDS_PATH = Path(\"/kaggle/input/petfinder-train-folds/train_folds.csv\") \n\ntrain_csv = COMP_PATH / \"train.csv\"\ntest_csv = COMP_PATH / \"test.csv\"\n\nprint(\"train exists:\", train_csv.exists())\nprint(\"test exists :\", test_csv.exists())\nprint(\"folds exists:\", FOLDS_PATH.exists())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:12:58.652539Z","iopub.execute_input":"2026-01-24T18:12:58.653039Z","iopub.status.idle":"2026-01-24T18:12:58.662592Z","shell.execute_reply.started":"2026-01-24T18:12:58.653011Z","shell.execute_reply":"2026-01-24T18:12:58.662010Z"}},"outputs":[{"name":"stdout","text":"train exists: True\ntest exists : True\nfolds exists: True\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"train_df = pd.read_csv(train_csv)\ntest_df  = pd.read_csv(test_csv)\nfolds_df = pd.read_csv(FOLDS_PATH)\n\nprint(\"Train shape:\", train_df.shape)\nprint(\"Test shape :\", test_df.shape)\nprint(\"Folds shape:\", folds_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:12:58.663560Z","iopub.execute_input":"2026-01-24T18:12:58.663904Z","iopub.status.idle":"2026-01-24T18:12:58.778980Z","shell.execute_reply.started":"2026-01-24T18:12:58.663874Z","shell.execute_reply":"2026-01-24T18:12:58.778210Z"}},"outputs":[{"name":"stdout","text":"Train shape: (6431, 3)\nTest shape : (1891, 2)\nFolds shape: (6431, 3)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:12:58.780720Z","iopub.execute_input":"2026-01-24T18:12:58.781038Z","iopub.status.idle":"2026-01-24T18:12:58.802734Z","shell.execute_reply.started":"2026-01-24T18:12:58.781017Z","shell.execute_reply":"2026-01-24T18:12:58.802002Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6431 entries, 0 to 6430\nData columns (total 3 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   PetID          6431 non-null   object\n 1   Description    6426 non-null   object\n 2   AdoptionSpeed  6431 non-null   int64 \ndtypes: int64(1), object(2)\nmemory usage: 150.9+ KB\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"test_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:12:58.803735Z","iopub.execute_input":"2026-01-24T18:12:58.804052Z","iopub.status.idle":"2026-01-24T18:12:58.811484Z","shell.execute_reply.started":"2026-01-24T18:12:58.804022Z","shell.execute_reply":"2026-01-24T18:12:58.810716Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1891 entries, 0 to 1890\nData columns (total 2 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   PetID        1891 non-null   object\n 1   Description  1890 non-null   object\ndtypes: object(2)\nmemory usage: 29.7+ KB\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"train_df[\"Description\"] = train_df[\"Description\"].fillna(\"\")\ntest_df[\"Description\"] = test_df[\"Description\"].fillna(\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:12:58.812492Z","iopub.execute_input":"2026-01-24T18:12:58.812861Z","iopub.status.idle":"2026-01-24T18:12:58.826979Z","shell.execute_reply.started":"2026-01-24T18:12:58.812813Z","shell.execute_reply":"2026-01-24T18:12:58.826259Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(train_df[\"AdoptionSpeed\"].value_counts().sort_index())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:12:58.827823Z","iopub.execute_input":"2026-01-24T18:12:58.828110Z","iopub.status.idle":"2026-01-24T18:12:58.841124Z","shell.execute_reply.started":"2026-01-24T18:12:58.828084Z","shell.execute_reply":"2026-01-24T18:12:58.840531Z"}},"outputs":[{"name":"stdout","text":"AdoptionSpeed\n1    1197\n2    1773\n3    1328\n4    2133\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(\"train dataset:\")\nprint(train_df.head(), \"\\n\")\nprint(\"test dataset:\")\nprint(test_df.head(), \"\\n\")\nprint(\"folds dataset:\")\nprint(folds_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:12:58.841833Z","iopub.execute_input":"2026-01-24T18:12:58.842170Z","iopub.status.idle":"2026-01-24T18:12:58.859222Z","shell.execute_reply.started":"2026-01-24T18:12:58.842142Z","shell.execute_reply":"2026-01-24T18:12:58.858410Z"}},"outputs":[{"name":"stdout","text":"train dataset:\n       PetID                                        Description  AdoptionSpeed\n0  d3b4f29f8  Mayleen and Flo are two lovely adorable sister...              2\n1  e9dc82251  A total of 5 beautiful Tabbys available for ad...              2\n2  8111f6d4a  Two-and-a-half month old girl. Very manja and ...              2\n3  693a90fda  Neil is a healthy and active ~2-month-old fema...              2\n4  9d08c85ef  Gray kitten available for adoption in sungai p...              2 \n\ntest dataset:\n       PetID                                        Description\n0  6697a7f62  This cute little puppy is looking for a loving...\n1  23b64fe21  These 3 puppies was rescued from a mechanic sh...\n2  41e824cbe  Ara needs a forever home! Believe me, he's a r...\n3  6c3d7237b  i rescue this homeless dog 2 years ago but my ...\n4  97b0b5d92  We found him at a shopping mall at a very clea... \n\nfolds dataset:\n       PetID  AdoptionSpeed  fold\n0  d3b4f29f8              2     3\n1  e9dc82251              2     3\n2  8111f6d4a              2     2\n3  693a90fda              2     4\n4  9d08c85ef              2     3\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:12:58.860230Z","iopub.execute_input":"2026-01-24T18:12:58.860521Z","iopub.status.idle":"2026-01-24T18:12:58.918861Z","shell.execute_reply.started":"2026-01-24T18:12:58.860493Z","shell.execute_reply":"2026-01-24T18:12:58.917727Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"model_name = \"bert-base-cased\"\nnum_classes = 4\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:12:58.920037Z","iopub.execute_input":"2026-01-24T18:12:58.920598Z","iopub.status.idle":"2026-01-24T18:13:24.354537Z","shell.execute_reply.started":"2026-01-24T18:12:58.920569Z","shell.execute_reply":"2026-01-24T18:13:24.353684Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39e334c250b4470b944df42aea12d635"}},"metadata":{}},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769278382.170015      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769278382.228060      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769278382.690905      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769278382.690946      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769278382.690948      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769278382.690950      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c772aabee3924467afc7fc185ba033e2"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaf55f7baf7645e3b29139243644dd6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c6313fc731949c39676c7b0cf3349c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67d0a854c152461293ccbc5d74f68267"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"text = train_df.loc[0, \"Description\"]  \ninputs = tokenizer(text)\ntokenizer.decode(inputs[\"input_ids\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:13:24.355560Z","iopub.execute_input":"2026-01-24T18:13:24.356139Z","iopub.status.idle":"2026-01-24T18:13:24.365127Z","shell.execute_reply.started":"2026-01-24T18:13:24.356116Z","shell.execute_reply":"2026-01-24T18:13:24.364535Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'[CLS] Mayleen and Flo are two lovely adorable sisters. They are very friendly and affectionate, but wary of strangers and make good watchdogs. Mayleen has golden hues on her face, making her a husky look - alike. Flo has a darker face with brown feet, and is the more outgoing and dominat of the two. Looking for good homes. Adopters must vaccinate and spay them. [SEP]'"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"inputs = tokenizer( \n    text, \n    max_length=200, \n    truncation=True,\n    return_overflowing_tokens=True, \n) \n  \nfor ids in inputs[\"input_ids\"]: \n    print(tokenizer.decode(ids)); print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:13:24.365907Z","iopub.execute_input":"2026-01-24T18:13:24.366164Z","iopub.status.idle":"2026-01-24T18:13:24.410294Z","shell.execute_reply.started":"2026-01-24T18:13:24.366132Z","shell.execute_reply":"2026-01-24T18:13:24.409421Z"}},"outputs":[{"name":"stdout","text":"[CLS] Mayleen and Flo are two lovely adorable sisters. They are very friendly and affectionate, but wary of strangers and make good watchdogs. Mayleen has golden hues on her face, making her a husky look - alike. Flo has a darker face with brown feet, and is the more outgoing and dominat of the two. Looking for good homes. Adopters must vaccinate and spay them. [SEP]\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"inputs = tokenizer( \n    text, \n    max_length=200, \n    truncation=True, \n    return_overflowing_tokens=True, \n    return_offsets_mapping=True, \n ) \ninputs.keys()\nprint(inputs.keys())\nprint(\"num chunks:\", len(inputs[\"input_ids\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:13:24.412964Z","iopub.execute_input":"2026-01-24T18:13:24.413246Z","iopub.status.idle":"2026-01-24T18:13:24.424702Z","shell.execute_reply.started":"2026-01-24T18:13:24.413225Z","shell.execute_reply":"2026-01-24T18:13:24.424092Z"}},"outputs":[{"name":"stdout","text":"KeysView({'input_ids': [[101, 1318, 21180, 1105, 143, 2858, 1132, 1160, 9020, 27627, 5919, 119, 1220, 1132, 1304, 4931, 1105, 12721, 2193, 117, 1133, 16970, 1104, 15712, 1105, 1294, 1363, 2824, 14082, 1116, 119, 1318, 21180, 1144, 5404, 177, 10589, 1113, 1123, 1339, 117, 1543, 1123, 170, 24418, 1440, 118, 11609, 119, 143, 2858, 1144, 170, 9934, 1339, 1114, 3058, 1623, 117, 1105, 1110, 1103, 1167, 25194, 1105, 1202, 14503, 1204, 1104, 1103, 1160, 119, 8540, 1111, 1363, 4481, 119, 24930, 4184, 5759, 1538, 191, 7409, 16430, 2193, 1105, 22620, 1183, 1172, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'offset_mapping': [[(0, 0), (0, 3), (3, 7), (8, 11), (12, 13), (13, 15), (16, 19), (20, 23), (24, 30), (31, 39), (40, 47), (47, 48), (49, 53), (54, 57), (58, 62), (63, 71), (72, 75), (76, 85), (85, 88), (88, 89), (90, 93), (94, 98), (99, 101), (102, 111), (112, 115), (116, 120), (121, 125), (126, 131), (131, 134), (134, 135), (135, 136), (137, 140), (140, 144), (145, 148), (149, 155), (156, 157), (157, 160), (161, 163), (164, 167), (168, 172), (172, 173), (174, 180), (181, 184), (185, 186), (187, 192), (193, 197), (197, 198), (198, 203), (203, 204), (205, 206), (206, 208), (209, 212), (213, 214), (215, 221), (222, 226), (227, 231), (232, 237), (238, 242), (242, 243), (244, 247), (248, 250), (251, 254), (255, 259), (260, 268), (269, 272), (273, 275), (275, 279), (279, 280), (281, 283), (284, 287), (288, 291), (291, 292), (293, 300), (301, 304), (305, 309), (310, 315), (315, 316), (317, 319), (319, 321), (321, 325), (326, 330), (331, 332), (332, 334), (334, 337), (337, 340), (341, 344), (345, 348), (348, 349), (350, 354), (354, 355), (0, 0)]], 'overflow_to_sample_mapping': [0]})\nnum chunks: 1\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"lengths = train_df[\"Description\"].fillna(\"\").astype(str).apply(\n    lambda x: len(tokenizer(x, add_special_tokens=True, truncation=False)[\"input_ids\"])\n)\n\nprint(\"count:\", lengths.shape[0])\nprint(\"min  :\", lengths.min())\nprint(\"p50  :\", int(lengths.quantile(0.50)))\nprint(\"p90  :\", int(lengths.quantile(0.90)))\nprint(\"p95  :\", int(lengths.quantile(0.95)))\nprint(\"p99  :\", int(lengths.quantile(0.99)))\nprint(\"max  :\", lengths.max())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:13:24.425609Z","iopub.execute_input":"2026-01-24T18:13:24.425893Z","iopub.status.idle":"2026-01-24T18:13:26.023541Z","shell.execute_reply.started":"2026-01-24T18:13:24.425867Z","shell.execute_reply":"2026-01-24T18:13:26.022910Z"}},"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1173 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"count: 6431\nmin  : 2\np50  : 66\np90  : 199\np95  : 287\np99  : 557\nmax  : 1487\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"class PetFinderDataset(Dataset):\n    def __init__(self, texts, labels=None, max_length=287):\n        self.texts = texts\n        self.labels = labels\n        self.max_length = max_length\n       \n    def __len__(self):\n        return len(self.texts)\n   \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        inputs = tokenizer(\n            text,\n            max_length=self.max_length,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n       \n        item = {key: val.squeeze(0) for key, val in inputs.items()}\n       \n        if self.labels is not None:\n            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n       \n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:13:26.024467Z","iopub.execute_input":"2026-01-24T18:13:26.024745Z","iopub.status.idle":"2026-01-24T18:13:26.030591Z","shell.execute_reply.started":"2026-01-24T18:13:26.024710Z","shell.execute_reply":"2026-01-24T18:13:26.030034Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"batch_size = 16\nmax_length = 288\nn_folds = 5\nnum_workers = 0 \nepochs = 4\nlr = 2e-5\nweight_decay=0.01\npin_memory = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:13:26.031562Z","iopub.execute_input":"2026-01-24T18:13:26.031832Z","iopub.status.idle":"2026-01-24T18:13:26.043421Z","shell.execute_reply.started":"2026-01-24T18:13:26.031804Z","shell.execute_reply":"2026-01-24T18:13:26.042767Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"master_df = train_df.merge(\n    folds_df[[\"PetID\", \"fold\"]],\n    on=\"PetID\",\n    how=\"left\"\n)\nprint(\"shape:\", master_df.shape)\nprint(\"missing fold:\", master_df[\"fold\"].isna().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:13:26.044244Z","iopub.execute_input":"2026-01-24T18:13:26.044965Z","iopub.status.idle":"2026-01-24T18:13:26.065677Z","shell.execute_reply.started":"2026-01-24T18:13:26.044944Z","shell.execute_reply":"2026-01-24T18:13:26.064994Z"}},"outputs":[{"name":"stdout","text":"shape: (6431, 4)\nmissing fold: 0\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"master_df[\"label\"] = master_df[\"AdoptionSpeed\"] - 1  # Convert to 0-3\nprint(master_df[\"AdoptionSpeed\"].min(), master_df[\"AdoptionSpeed\"].max())\nprint(master_df[\"label\"].min(), master_df[\"label\"].max())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:13:26.066508Z","iopub.execute_input":"2026-01-24T18:13:26.066768Z","iopub.status.idle":"2026-01-24T18:13:26.072824Z","shell.execute_reply.started":"2026-01-24T18:13:26.066747Z","shell.execute_reply":"2026-01-24T18:13:26.072171Z"}},"outputs":[{"name":"stdout","text":"1 4\n0 3\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"for fold in range(n_folds):\n    print(\"№ Fold:\", fold)\n\n    train = master_df[master_df[\"fold\"] != fold]\n    val = master_df[master_df[\"fold\"] == fold]\n\n    train_texts = train[\"Description\"].values\n    train_labels = train[\"label\"].values\n\n    val_texts = val[\"Description\"].values\n    val_labels = val[\"label\"].values\n\n    train_dataset = PetFinderDataset(\n        texts=train_texts,\n        labels=train_labels,\n        max_length=max_length\n    )\n\n    val_dataset = PetFinderDataset(\n        texts=val_texts,\n        labels=val_labels,\n        max_length=max_length\n    )\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=pin_memory\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=pin_memory\n    )\n\n    print(\"Train batches:\", len(train_loader))\n    print(\" Val batches:\", len(val_loader), \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:13:26.073646Z","iopub.execute_input":"2026-01-24T18:13:26.073916Z","iopub.status.idle":"2026-01-24T18:13:26.095459Z","shell.execute_reply.started":"2026-01-24T18:13:26.073888Z","shell.execute_reply":"2026-01-24T18:13:26.094876Z"}},"outputs":[{"name":"stdout","text":"№ Fold: 0\nTrain batches: 322\n Val batches: 81 \n\n№ Fold: 1\nTrain batches: 322\n Val batches: 81 \n\n№ Fold: 2\nTrain batches: 322\n Val batches: 81 \n\n№ Fold: 3\nTrain batches: 322\n Val batches: 81 \n\n№ Fold: 4\nTrain batches: 322\n Val batches: 81 \n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"batch = next(iter(train_loader))\n\nfor k, v in batch.items():\n    print(k, v.shape, v.dtype)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:13:26.096359Z","iopub.execute_input":"2026-01-24T18:13:26.096661Z","iopub.status.idle":"2026-01-24T18:13:26.122739Z","shell.execute_reply.started":"2026-01-24T18:13:26.096639Z","shell.execute_reply":"2026-01-24T18:13:26.122046Z"}},"outputs":[{"name":"stdout","text":"input_ids torch.Size([16, 288]) torch.int64\ntoken_type_ids torch.Size([16, 288]) torch.int64\nattention_mask torch.Size([16, 288]) torch.int64\nlabels torch.Size([16]) torch.int64\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(\"labels min/max:\", batch[\"labels\"].min().item(), batch[\"labels\"].max().item())\nprint(\"unique (first 50):\", torch.unique(batch[\"labels\"])[:50])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:13:26.123707Z","iopub.execute_input":"2026-01-24T18:13:26.123964Z","iopub.status.idle":"2026-01-24T18:13:26.140796Z","shell.execute_reply.started":"2026-01-24T18:13:26.123944Z","shell.execute_reply":"2026-01-24T18:13:26.140275Z"}},"outputs":[{"name":"stdout","text":"labels min/max: 0 3\nunique (first 50): tensor([0, 1, 2, 3])\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"model.train()\noptimizer = AdamW(model.parameters(), lr=lr)\n\nbatch = next(iter(train_loader))\noutputs = model(\n    input_ids=batch[\"input_ids\"].to(device),\n    attention_mask=batch[\"attention_mask\"].to(device),\n    labels=batch[\"labels\"].to(device),\n)\nloss = outputs.loss\n\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n\nprint(\"train step loss:\", loss.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:13:26.141590Z","iopub.execute_input":"2026-01-24T18:13:26.141876Z","iopub.status.idle":"2026-01-24T18:13:27.290124Z","shell.execute_reply.started":"2026-01-24T18:13:26.141829Z","shell.execute_reply":"2026-01-24T18:13:27.289332Z"}},"outputs":[{"name":"stdout","text":"train step loss: 1.3669534921646118\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"model.eval()\n\nwith torch.no_grad():\n    outputs = model(\n        input_ids=batch[\"input_ids\"].to(device),\n        attention_mask=batch[\"attention_mask\"].to(device),\n        labels=batch[\"labels\"].to(device),\n    )\n\nprint(\"loss:\", outputs.loss)\nprint(\"logits shape:\", outputs.logits.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:13:27.291195Z","iopub.execute_input":"2026-01-24T18:13:27.291492Z","iopub.status.idle":"2026-01-24T18:13:27.607523Z","shell.execute_reply.started":"2026-01-24T18:13:27.291464Z","shell.execute_reply":"2026-01-24T18:13:27.606749Z"}},"outputs":[{"name":"stdout","text":"loss: tensor(1.2724, device='cuda:0')\nlogits shape: torch.Size([16, 4])\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, optimizer, device, scheduler=None):\n    pbar = tqdm(train_loader, desc=\"Training\")\n    model.train()\n    total_loss = 0.0\n\n    for batch in pbar:\n        outputs = model(\n            input_ids=batch[\"input_ids\"].to(device),\n            attention_mask=batch[\"attention_mask\"].to(device),\n            labels=batch[\"labels\"].to(device),\n        )\n        loss = outputs.loss\n        optimizer.zero_grad()\n        loss.backward()\n        if scheduler:\n            scheduler.step()\n        optimizer.step()\n        total_loss += loss.item()\n        avg_so_far = total_loss / (pbar.n + 1)\n        pbar.set_postfix(loss=f\"{loss.item():.4f}\", avg=f\"{avg_so_far:.4f}\")\n    avg_loss = total_loss / len(train_loader)\n    return avg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:13:27.608618Z","iopub.execute_input":"2026-01-24T18:13:27.609042Z","iopub.status.idle":"2026-01-24T18:13:27.614373Z","shell.execute_reply.started":"2026-01-24T18:13:27.609010Z","shell.execute_reply":"2026-01-24T18:13:27.613742Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"@torch.no_grad()\ndef validate_one_epoch(model, val_loader, device):\n    model.eval()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n\n    pbar = tqdm(val_loader, desc=\"Validating\")\n\n    with torch.no_grad():\n        for batch in pbar:\n            outputs = model(\n                input_ids=batch[\"input_ids\"].to(device),\n                attention_mask=batch[\"attention_mask\"].to(device),\n                labels=batch[\"labels\"].to(device),\n            )\n\n            loss = outputs.loss\n            logits = outputs.logits\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(logits, dim=1).cpu().numpy()   # 0..3\n            labels = batch[\"labels\"].cpu().numpy()              # 0..3\n\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n\n            avg_so_far = total_loss / (pbar.n + 1)\n            pbar.set_postfix(avg_loss=f\"{avg_so_far:.4f}\")\n\n    avg_loss = total_loss / len(val_loader)\n\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n\n    qwk = cohen_kappa_score(all_labels + 1, all_preds + 1, weights=\"quadratic\")\n\n    return avg_loss, qwk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:13:27.615482Z","iopub.execute_input":"2026-01-24T18:13:27.615934Z","iopub.status.idle":"2026-01-24T18:13:27.627522Z","shell.execute_reply.started":"2026-01-24T18:13:27.615913Z","shell.execute_reply":"2026-01-24T18:13:27.626893Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"WEIGHTS_DIR = Path(\"/kaggle/working/artifacts\")\nWEIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n\nfor fold in range(n_folds):\n    print(f\"\\n===== FOLD {fold} =====\")\n\n    train = master_df[master_df[\"fold\"] != fold].reset_index(drop=True)\n    val   = master_df[master_df[\"fold\"] == fold].reset_index(drop=True)\n\n    train_texts  = train[\"Description\"].values\n    train_labels = train[\"label\"].values\n\n    val_texts  = val[\"Description\"].values\n    val_labels = val[\"label\"].values\n\n    train_dataset = PetFinderDataset(train_texts, train_labels, max_length=max_length)\n    val_dataset   = PetFinderDataset(val_texts,   val_labels,   max_length=max_length)\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n    )\n    print(\"Train batches:\", len(train_loader))\n    print(\" Val batches:\", len(val_loader))\n    model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=4).to(device)\n    optimizer = AdamW(model.parameters(), lr=lr,weight_decay=weight_decay)\n\n    best_qwk = -1e9\n    best_path = f\"{WEIGHTS_DIR}/nlp_{model_name}_fold{fold}.pth\"\n\n    # 6) epochs loop\n    for epoch in range(epochs): #, epochs + 1\n        print(f\"\\n--- Epoch {epoch}/{epochs} ---\")\n\n        train_loss = train_one_epoch(model, train_loader, optimizer, device)  # твоя функція\n        val_loss, val_qwk = validate_one_epoch(model, val_loader, device)     # нова функція\n\n        print(f\"train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | val_QWK: {val_qwk:.4f}\")\n\n        # 7) save best weights\n        if val_qwk > best_qwk:\n            best_qwk = val_qwk\n            torch.save(model.state_dict(), best_path)\n            print(f\"✅ Saved best: {best_path} | best_QWK={best_qwk:.4f}\")\n\n    print(f\"\\nFOLD {fold} DONE | best_QWK={best_qwk:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:13:27.628347Z","iopub.execute_input":"2026-01-24T18:13:27.628571Z","iopub.status.idle":"2026-01-24T19:35:34.532192Z","shell.execute_reply.started":"2026-01-24T18:13:27.628552Z","shell.execute_reply":"2026-01-24T19:35:34.531306Z"}},"outputs":[{"name":"stdout","text":"\n===== FOLD 0 =====\nTrain batches: 322\n Val batches: 81\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 0/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:29<00:00,  2.16it/s, avg=1.3548, loss=1.1107]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.84it/s, avg_loss=1.3244]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.3548 | val_loss: 1.3244 | val_QWK: 0.0784\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold0.pth | best_QWK=0.0784\n\n--- Epoch 1/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=1.3065, loss=1.3584]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.82it/s, avg_loss=1.3091]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.3065 | val_loss: 1.3091 | val_QWK: 0.2506\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold0.pth | best_QWK=0.2506\n\n--- Epoch 2/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.13it/s, avg=1.2053, loss=1.3368]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.83it/s, avg_loss=1.3276]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.2053 | val_loss: 1.3276 | val_QWK: 0.2663\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold0.pth | best_QWK=0.2663\n\n--- Epoch 3/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=1.0063, loss=0.9027]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.82it/s, avg_loss=1.5226]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.0063 | val_loss: 1.5226 | val_QWK: 0.2572\n\n--- Epoch 4/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=0.7202, loss=0.6113]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.83it/s, avg_loss=1.6085]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 0.7202 | val_loss: 1.6085 | val_QWK: 0.2893\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold0.pth | best_QWK=0.2893\n\n--- Epoch 5/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=0.4398, loss=0.4131]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.82it/s, avg_loss=2.1877]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 0.4398 | val_loss: 2.1877 | val_QWK: 0.2478\n\nFOLD 0 DONE | best_QWK=0.2893\n\n===== FOLD 1 =====\nTrain batches: 322\n Val batches: 81\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 0/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=1.3519, loss=1.3951]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.84it/s, avg_loss=1.2973]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.3519 | val_loss: 1.2973 | val_QWK: 0.1381\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold1.pth | best_QWK=0.1381\n\n--- Epoch 1/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=1.2733, loss=1.2343]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.83it/s, avg_loss=1.2821]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.2733 | val_loss: 1.2821 | val_QWK: 0.2009\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold1.pth | best_QWK=0.2009\n\n--- Epoch 2/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=1.1354, loss=1.1807]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.83it/s, avg_loss=1.3134]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.1354 | val_loss: 1.3134 | val_QWK: 0.2804\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold1.pth | best_QWK=0.2804\n\n--- Epoch 3/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=0.8508, loss=1.2345]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.84it/s, avg_loss=1.4734]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 0.8508 | val_loss: 1.4734 | val_QWK: 0.2917\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold1.pth | best_QWK=0.2917\n\n--- Epoch 4/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=0.5571, loss=0.6466]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.84it/s, avg_loss=1.9340]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 0.5571 | val_loss: 1.9340 | val_QWK: 0.2707\n\n--- Epoch 5/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=0.3499, loss=0.4673]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.83it/s, avg_loss=2.1277]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 0.3499 | val_loss: 2.1277 | val_QWK: 0.2492\n\nFOLD 1 DONE | best_QWK=0.2917\n\n===== FOLD 2 =====\nTrain batches: 322\n Val batches: 81\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 0/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=1.3484, loss=1.0790]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.82it/s, avg_loss=1.3221]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.3484 | val_loss: 1.3221 | val_QWK: 0.1599\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold2.pth | best_QWK=0.1599\n\n--- Epoch 1/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=1.2481, loss=1.3283]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.82it/s, avg_loss=1.2843]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.2481 | val_loss: 1.2843 | val_QWK: 0.2456\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold2.pth | best_QWK=0.2456\n\n--- Epoch 2/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=1.0809, loss=0.9560]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.82it/s, avg_loss=1.3101]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.0809 | val_loss: 1.3101 | val_QWK: 0.2972\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold2.pth | best_QWK=0.2972\n\n--- Epoch 3/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=0.8084, loss=0.9655]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.83it/s, avg_loss=1.4732]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 0.8084 | val_loss: 1.4732 | val_QWK: 0.2813\n\n--- Epoch 4/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=0.5361, loss=0.1739]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.82it/s, avg_loss=1.8062]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 0.5361 | val_loss: 1.8062 | val_QWK: 0.2858\n\n--- Epoch 5/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=0.3165, loss=0.1095]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.81it/s, avg_loss=2.2364]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 0.3165 | val_loss: 2.2364 | val_QWK: 0.2260\n\nFOLD 2 DONE | best_QWK=0.2972\n\n===== FOLD 3 =====\nTrain batches: 322\n Val batches: 81\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 0/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=1.3625, loss=1.3971]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.81it/s, avg_loss=1.3264]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.3625 | val_loss: 1.3264 | val_QWK: 0.1442\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold3.pth | best_QWK=0.1442\n\n--- Epoch 1/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=1.3011, loss=1.4656]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.83it/s, avg_loss=1.2976]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.3011 | val_loss: 1.2976 | val_QWK: 0.1894\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold3.pth | best_QWK=0.1894\n\n--- Epoch 2/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=1.2107, loss=1.5260]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.82it/s, avg_loss=1.2840]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.2107 | val_loss: 1.2840 | val_QWK: 0.2603\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold3.pth | best_QWK=0.2603\n\n--- Epoch 3/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=1.0292, loss=1.0734]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.81it/s, avg_loss=1.3210]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.0292 | val_loss: 1.3210 | val_QWK: 0.2752\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold3.pth | best_QWK=0.2752\n\n--- Epoch 4/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=0.7461, loss=0.7172]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.82it/s, avg_loss=1.8774]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 0.7461 | val_loss: 1.8774 | val_QWK: 0.2347\n\n--- Epoch 5/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=0.5045, loss=0.6461]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.81it/s, avg_loss=1.7750]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 0.5045 | val_loss: 1.7750 | val_QWK: 0.2369\n\nFOLD 3 DONE | best_QWK=0.2752\n\n===== FOLD 4 =====\nTrain batches: 322\n Val batches: 81\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 0/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=1.3531, loss=1.4997]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.81it/s, avg_loss=1.3193]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.3531 | val_loss: 1.3193 | val_QWK: 0.1560\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold4.pth | best_QWK=0.1560\n\n--- Epoch 1/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=1.2680, loss=0.9825]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.83it/s, avg_loss=1.3046]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.2680 | val_loss: 1.3046 | val_QWK: 0.2267\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold4.pth | best_QWK=0.2267\n\n--- Epoch 2/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:32<00:00,  2.12it/s, avg=1.1406, loss=0.8980]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.81it/s, avg_loss=1.3214]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.1406 | val_loss: 1.3214 | val_QWK: 0.2569\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold4.pth | best_QWK=0.2569\n\n--- Epoch 3/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:32<00:00,  2.12it/s, avg=0.8938, loss=1.1866]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.82it/s, avg_loss=1.4228]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 0.8938 | val_loss: 1.4228 | val_QWK: 0.2865\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold4.pth | best_QWK=0.2865\n\n--- Epoch 4/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:32<00:00,  2.12it/s, avg=0.6183, loss=0.1730]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.81it/s, avg_loss=1.7500]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 0.6183 | val_loss: 1.7500 | val_QWK: 0.2933\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold4.pth | best_QWK=0.2933\n\n--- Epoch 5/6 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 322/322 [02:31<00:00,  2.12it/s, avg=0.3879, loss=0.3165]\nValidating: 100%|██████████| 81/81 [00:11<00:00,  6.82it/s, avg_loss=1.9869]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 0.3879 | val_loss: 1.9869 | val_QWK: 0.3050\n✅ Saved best: /kaggle/working/artifacts/nlp_bert-base-cased_fold4.pth | best_QWK=0.3050\n\nFOLD 4 DONE | best_QWK=0.3050\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"ART = Path(\"/kaggle/working/artifacts\")\nART.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T19:41:32.992116Z","iopub.execute_input":"2026-01-24T19:41:32.992726Z","iopub.status.idle":"2026-01-24T19:41:32.996352Z","shell.execute_reply.started":"2026-01-24T19:41:32.992699Z","shell.execute_reply":"2026-01-24T19:41:32.995737Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"oof = np.zeros((len(master_df), 4), dtype=np.float32)\n\nfor fold in range(n_folds):\n    val = master_df[master_df.fold == fold].reset_index()\n    val_dataset = PetFinderDataset(val[\"Description\"].values, val[\"label\"].values, max_length=max_length)\n    val_loader  = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4).to(device)\n    model.load_state_dict(torch.load(ART / f\"nlp_{model_name}_fold{fold}.pth\", map_location=device))\n    model.eval()\n\n    probs = []\n    with torch.no_grad():\n        for b in val_loader:\n            out = model(input_ids=b[\"input_ids\"].to(device), attention_mask=b[\"attention_mask\"].to(device))\n            probs.append(torch.softmax(out.logits, 1).cpu().numpy())\n    oof[val[\"index\"].values] = np.vstack(probs)\n\noof_df = pd.DataFrame({\"PetID\": master_df[\"PetID\"].values})\nfor i in range(4): oof_df[f\"nlp_proba_{i+1}\"] = oof[:, i]\noof_df[\"nlp_pred\"] = np.argmax(oof, 1) + 1\noof_df.to_csv(ART / \"nlp_oof.csv\", index=False)\nprint(\"saved:\", ART / \"nlp_oof.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T19:41:35.698616Z","iopub.execute_input":"2026-01-24T19:41:35.698942Z","iopub.status.idle":"2026-01-24T19:42:37.040347Z","shell.execute_reply.started":"2026-01-24T19:41:35.698915Z","shell.execute_reply":"2026-01-24T19:42:37.039665Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"saved: /kaggle/working/artifacts/nlp_oof.csv\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"test_dataset = PetFinderDataset(test_df[\"Description\"].fillna(\"\").values, labels=None, max_length=max_length)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n\ntest_probs = np.zeros((len(test_df), 4), dtype=np.float32)\n\nfor fold in range(n_folds):\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4).to(device)\n    model.load_state_dict(torch.load(ART / f\"nlp_{model_name}_fold{fold}.pth\", map_location=device))\n    model.eval()\n\n    probs = []\n    with torch.no_grad():\n        for b in test_loader:\n            out = model(input_ids=b[\"input_ids\"].to(device), attention_mask=b[\"attention_mask\"].to(device))\n            probs.append(torch.softmax(out.logits, 1).cpu().numpy())\n    test_probs += np.vstack(probs)\n\ntest_probs /= n_folds\n\ntest_out = pd.DataFrame({\"PetID\": test_df[\"PetID\"].values})\nfor i in range(4): test_out[f\"nlp_proba_{i+1}\"] = test_probs[:, i]\ntest_out[\"nlp_pred\"] = np.argmax(test_probs, 1) + 1\ntest_out.to_csv(ART / \"nlp_test.csv\", index=False)\nprint(\"saved:\", ART / \"nlp_test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T19:44:06.213451Z","iopub.execute_input":"2026-01-24T19:44:06.213681Z","iopub.status.idle":"2026-01-24T19:45:35.394727Z","shell.execute_reply.started":"2026-01-24T19:44:06.213661Z","shell.execute_reply":"2026-01-24T19:45:35.393900Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"saved: /kaggle/working/artifacts/nlp_test.csv\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"!zip -r artifacts.zip /kaggle/working/artifacts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T19:53:11.811156Z","iopub.execute_input":"2026-01-24T19:53:11.811858Z","iopub.status.idle":"2026-01-24T19:55:02.777903Z","shell.execute_reply.started":"2026-01-24T19:53:11.811815Z","shell.execute_reply":"2026-01-24T19:55:02.777014Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/artifacts/ (stored 0%)\n  adding: kaggle/working/artifacts/nlp_bert-base-cased_fold2.pth","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 7%)\n  adding: kaggle/working/artifacts/nlp_bert-base-cased_fold4.pth (deflated 7%)\n  adding: kaggle/working/artifacts/nlp_bert-base-cased_fold3.pth (deflated 7%)\n  adding: kaggle/working/artifacts/nlp_bert-base-cased_fold1.pth (deflated 7%)\n  adding: kaggle/working/artifacts/nlp_oof.csv (deflated 53%)\n  adding: kaggle/working/artifacts/nlp_bert-base-cased_fold0.pth (deflated 7%)\n  adding: kaggle/working/artifacts/nlp_test.csv (deflated 53%)\n","output_type":"stream"}],"execution_count":35}]}